{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# This script serves as example to parsing the txt alignments. It will iterate over all \n",
    "# utterances of the dataset, split utterances on silences that are longer 0.4s and play them.\n",
    "# (this does not change your files)\n",
    "\n",
    "librispeech_root = \"./LibriSpeech\"    # Replace with yours\n",
    "\n",
    "def split_on_silences(audio_fpath, words, end_times):\n",
    "    # Load the audio waveform\n",
    "    sample_rate = 16000     # Sampling rate of LibriSpeech \n",
    "    wav, _ = librosa.load(audio_fpath, sample_rate)\n",
    "    \n",
    "    words = np.array(words)\n",
    "    start_times = np.array([0.0] + end_times[:-1])\n",
    "    end_times = np.array(end_times)\n",
    "    assert len(words) == len(end_times) == len(start_times)\n",
    "    assert words[0] == '' and words[-1] == ''\n",
    "    \n",
    "\n",
    "    wavs = [wav[(start_times[t]*sample_rate).astype(np.int):(end_times[t]*sample_rate).astype(np.int)] for t in range(len(words))]\n",
    "    texts = [word for word in words]\n",
    "    \n",
    "\n",
    "    \n",
    "    return wavs, texts\n",
    "\n",
    "\n",
    "w=[]\n",
    "t=[]\n",
    "# Select sets (e.g. dev-clean, train-other-500, ...)\n",
    "for set_name in ['demo_clean']:\n",
    "    set_dir = os.path.join(librispeech_root, set_name)\n",
    "    if not os.path.isdir(set_dir):\n",
    "        continue\n",
    "    \n",
    "    # Select speakers\n",
    "    for speaker_id in os.listdir(set_dir):\n",
    "        speaker_dir = os.path.join(set_dir, speaker_id)\n",
    "        \n",
    "        # Select books\n",
    "        for book_id in os.listdir(speaker_dir):\n",
    "            book_dir = os.path.join(speaker_dir, book_id)\n",
    "            \n",
    "            # Get the alignment file\n",
    "            alignment_fpath = os.path.join(book_dir, \"%s-%s.alignment.txt\" % \n",
    "                                            (speaker_id, book_id))\n",
    "            if not os.path.exists(alignment_fpath):\n",
    "                raise Exception(\"Alignment file not found. Did you download and merge the txt \"\n",
    "                                \"alignments with your LibriSpeech dataset?\")\n",
    "            \n",
    "            # Parse each utterance present in the file\n",
    "            alignment_file = open(alignment_fpath, \"r\")\n",
    "            for line in alignment_file:\n",
    "                \n",
    "                # Retrieve the utterance id, the words as a list and the end_times as a list\n",
    "                utterance_id, words, end_times = line.strip().split(' ')\n",
    "                words = words.replace('\\\"', '').split(',')\n",
    "                end_times = [float(e) for e in end_times.replace('\\\"', '').split(',')]\n",
    "                audio_fpath = os.path.join(book_dir, utterance_id + '.flac')\n",
    "                \n",
    "                # Split utterances on silences\n",
    "                wavs, texts = split_on_silences(audio_fpath, words, end_times)\n",
    "                w.append(wavs)\n",
    "                t.append(t)\n",
    "                \n",
    "            alignment_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/aniset/Speech\n\n"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4dfc7929655c4c7cbc725bd732c1ed4a",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}